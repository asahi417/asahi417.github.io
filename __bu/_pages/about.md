---
permalink: /
title: "ABOUT ME"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Ph.D student in the [School of Computer Science and Informatics](https://www.cardiff.ac.uk/computer-science) at Cardiff University,
co-advised by [Jose-camacho Collados](http://josecamachocollados.com/) and [Steven Schockaert](https://www.cardiff.ac.uk/people/view/133772-schockaert-steven).
Before joining Cardiff University, I had been a full-time researcher at [Cogent Labs](https://www.cogent.co.jp/en/) from 2018 to 2020. 
In 2021, I did research internships at Amazon supervised by [Danushka Bollegala](https://danushka.net/),
and Snapchat co-supervised by [Francesco Barbieri](https://research.snap.com/team/francesco-barbieri/) and 
[Leonardo Neves](https://research.snap.com/team/leonardo-neves/).

## Research
I study language models, especially the **relational knowledge** of the language models. 

Representative Papers:

- <b>Asahi Ushio</b>, Jose Camacho-Collados, and Steven Schockaert <br>
Distilling Relation Embeddings from Pre-trained Language Models  <br>
 <em>Proceedings of EMNLP 2021 Main Conference</em>
[<a href="https://aclanthology.org/2021.emnlp-main.712.pdf">pdf</a>]
[<a href="https://github.com/asahi417/relbert">code</a>]
[<a href="https://www.slideshare.net/asahiushio1/202111-emnlp-distilling-relation-embeddings-from-pretrained-language-models">slide</a>]

- <b>Asahi Ushio</b>, Luis Espinosa-Anke, Steven Schockaert, and Jose Camacho-Collados <br>
BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies? <br>
 <em>Proceedings of ACL-IJCNLP 2021 Main Conference</em>
[<a href="https://aclanthology.org/2021.acl-long.280.pdf">pdf</a>]
[<a href="https://github.com/asahi417/analogy-language-model">code</a>]
[<a href="https://www.slideshare.net/asahiushio1/202105-acl-bert-is-to-nlp-what-alexnet-is-to-cv-can-pretrained-language-models-identify-analogies">slide</a>]

