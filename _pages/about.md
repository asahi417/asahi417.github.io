---
permalink: /
excerpt: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hello! I am a Research Engineer at Google Research, London working on multi-media generation.
Previously, I was an applied scientist at Amazon working on information retrieval and product search, and 
a former Ph.D student in the [School of Computer Science and Informatics](https://www.cardiff.ac.uk/computer-science) at Cardiff University,
co-advised by [Jose-camacho Collados](http://josecamachocollados.com/) and [Steven Schockaert](https://www.cardiff.ac.uk/people/view/133772-schockaert-steven).
During my PhD, I studied **relational knowledge representation in language model** and 
application of language models in tasks such as 
**named-entity recognition** (eg. [T-NER](https://github.com/asahi417/tner), [TweetNER7](https://huggingface.co/datasets/tner/tweetner7)) 
and **question generation**. I also studied NLP on social media, and I am a part of [TweetNLP](https://tweetnlp.org/), where I am developing the [core library](https://github.com/cardiffnlp/tweetnlp). Aside from my role at Amazon, I am collaborating with [Kotoba-technologies](https://www.kotoba.tech) for research on bilingual speech foundation model of Japanese and English.

Representative Papers (see [full publication](https://asahiushio.com/publications)):
- <b>Asahi Ushio</b>, Jose Camacho-Collados, and Steven Schockaert <br>
Distilling Relation Embeddings from Pre-trained Language Models  <br>
 <em>Proceedings of EMNLP 2021 Main Conference</em>
<a href="https://aclanthology.org/2021.emnlp-main.712.pdf">[pdf]</a>
<a href="https://github.com/asahi417/relbert">[code]</a>
<a href="https://www.slideshare.net/asahiushio1/202111-emnlp-distilling-relation-embeddings-from-pretrained-language-models">[slide]</a>
<a href="https://aclanthology.org/2021.emnlp-main.712">[acl anthology]</a>
<a href="https://arxiv.org/abs/2110.15705">[arxiv]</a>
<a href="https://huggingface.co/spaces/relbert/Analogy">[demo]</a>
- <b>Asahi Ushio</b>, Luis Espinosa-Anke, Steven Schockaert, and Jose Camacho-Collados <br>
BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies? <br>
 <em>Proceedings of ACL-IJCNLP 2021 Main Conference</em>
<a href="https://aclanthology.org/2021.acl-long.280.pdf">[pdf]</a>
<a href="https://github.com/asahi417/analogy-language-model">[code]</a>
<a href="https://www.slideshare.net/asahiushio1/202105-acl-bert-is-to-nlp-what-alexnet-is-to-cv-can-pretrained-language-models-identify-analogies">[slide]</a>
<a href="https://aclanthology.org/2021.acl-long.280">[acl anthology]</a>
<a href="https://arxiv.org/abs/2105.04949">[arxiv]</a>


Open Source Projects:
- [***T-NER***](https://github.com/asahi417/tner): A python library to facilitate named-entity-recognition fine-tuning, evaluation, and inference via API.
- [***LMQG***](https://github.com/asahi417/lm-question-generation): Web application to run multi-lingual question generation models.
- [***TweetNLP***](https://github.com/cardiffnlp/tweetnlp): A python library of comprehensive NLP solutions tailored for Twitter.
- [***KEX***](https://pypi.org/project/kex): A python library of modern graph-based keyphrase extraction.

In 2023, I did a research internship at [Google Research](https://research.google/) working on MusicLM team supervised by [Andrea Agostinelli](https://scholar.google.it/citations?user=NM85zIEAAAAJ&hl=en). In 2021, I did research internships at Amazon supervised by [Danushka Bollegala](https://danushka.net/), and Snapchat co-supervised by [Francesco Barbieri](https://research.snap.com/team/francesco-barbieri/), [VÃ­tor Silva Sousa](https://research.snap.com/team/vitor-silva-sousa), and
[Leonardo Neves](https://research.snap.com/team/leonardo-neves/).
Before joining Cardiff University, I had been a full-time research engineer at [Cogent Labs](https://www.cogent.co.jp/en/) from 2018 to 2020.
Aside from NLP, I spend some time on the research of **computational art** ([WikiART Face](https://asahi417.github.io/projects/wikiart_face/)).
